{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create your own Word2Vec model for your domain üß†\n",
    "This tutorial will cover:\n",
    "- What are word embeddings.\n",
    "- Applications for word embeddings.\n",
    "- How you can train your own word embedding model (Word2Vec) using the Python library Gensim.\n",
    "- Text cleaning methods & considering your domain.\n",
    "- A wee tiny bit of linguistics.\n",
    "\n",
    "We will be training a Word2Vec model on scientific abstracts taken from the Semantic Scholar Graph API! This process is very similar to the work I am doing in building [ClimateScholar](https://github.com/EarthNLP/ClimateScholar/) (an open source climate science literature search engine).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are word embeddings? ü§î\n",
    "\n",
    "\n",
    "## Why should you care? ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Gensim? ü§î\n",
    "\n",
    "[Gensim](https://radimrehurek.com/gensim/) is a Python library developed by [RaRe Technologies](https://rare-technologies.com/) that makes training a wide variety of topic models very easy. \n",
    "\n",
    "## Tutorial versions üî¢\n",
    "Just so you can follow along for reproducibility reasons, here are the versions for Python & the packages:\n",
    "- Python:\n",
    "- Gensim:\n",
    "- Jupyter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing our data üë©üèΩ‚Äçüç≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of our imports for the tutorial\n",
    "# The core import for Word2Vec\n",
    "import gensim.models\n",
    "\n",
    "# We'll be using these imports during our text cleaning\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_multiple_whitespaces, strip_punctuation \n",
    "import re\n",
    "\n",
    "# Our data is stored in jsonl format\n",
    "import json\n",
    "\n",
    "# Used later to show off words from our trained model\n",
    "import random\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "root_path = \"./data/\"\n",
    "file_names = [\"weather_CO2.jsonl\", \"paleoclimate.jsonl\", \"rewilding.jsonl\", \"rockfish.jsonl\", \"arctic.jsonl\", \"climate.jsonl\", \"shark_climate.jsonl\"]\n",
    "\n",
    "for file_path in file_names:\n",
    "    with open(f'{root_path}/{file_path}', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    result = json.loads(json_list[0])\n",
    "\n",
    "    for result_dict in result[\"data\"]:\n",
    "        papers.append(result_dict)\n",
    "\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a peek at how our data looks!\n",
    "papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can condense this to a single list comprehension, but for readability I chose to keep them separate.\n",
    "# There are a lot of fields we don't need for this tutorial. We just want the abstract.\n",
    "# So all we're doing here is removing all entries in our data with a null abstract, and making a list of abstracts for easy processing.\n",
    "data = [y for y in (x for x in papers) if y[\"abstract\"] is not None]\n",
    "abstracts = [item['abstract'] for item in data]\n",
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should just see the abstract text now after the above processing.\n",
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded our data. We can move forward with the most important part of cleaning our data.\n",
    "\n",
    "### ‚ú®Preprocessing‚ú®\n",
    "\n",
    "This is where your domain specific knowledge comes in. Do not be fooled with how easy this seems! This is a process that takes some time to get right. Tutorials can trick you with how easy this might seem, but alas that is not the case. \n",
    "\n",
    "We're working with scientific text, so let us consider the goals of this process:\n",
    "- We want to keep scientific terms, so avoid removing any rare or uncommon words.\n",
    "- Stopwords (the, we, and, etc) can safely be removed because their inclusion doesn't impact the meaning of our data. It also means less words for our model to be trained on.\n",
    "- Since authors can be inconsistent when captializing words (co2 vs CO2). Let's lowercase everything so the same words aren't treated differently.\n",
    "- The punctuation looked at me funny. So let's remove it because we don't need it.\n",
    "- I am from the future, so I know that after perfoming some of the operations above, we ended up with a lot of whitespace. Let's make sure to remove any extra!\n",
    "- The last step will be to remove years from the data. We can achieve this with some simple regex below. For our usecase, years just add noise to the embedding space. \n",
    "\n",
    "\n",
    "### Exploring other ideas üöÄ\n",
    "There are a few other ways we can improve this process, we won't show them in the tutorial, but here are some ideas:\n",
    "\n",
    "- Phrase Matching: Climate Change is treated as two separate words (climate & change), but we often think of this phrase as a single \"concept\". Most NLP methods define a word as a string of letters, separated by whitespace. This method won't work for all languages, but it also doesn't work for all domains. \n",
    "    - Linguistics also has another definition for a word as anything that encaptures a single semantic concept. \n",
    "        - So in our case: Climate Change or Sebastes ruberrimus (the scientific name for Yellow Rockfish). You can combine them into a single word for processing by replacing the space with a hyphen or concat them together:\n",
    "            - Climate change becomes climate-change or climateChange, etc\n",
    "\n",
    "- You can use a statistical tokenizer (like the ones used by Transformer models) along with minor cleaning to prepare your text. (Not sure *why* you would do this, but you could!)\n",
    "\n",
    "Text cleaning is also a never \"finished\" step. You can forever tinker here to better capture your domain in the embedding space. We're going to consider this good enough for now though üòâ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_pattern = r'20[0-9][0-9]'\n",
    "def clean_sent(sent: str) -> List[str]:\n",
    "    # You can see the steps here are the ones we covered above!\n",
    "    removed_stopwords = remove_stopwords(sent)\n",
    "    lowered_string = removed_stopwords.lower()\n",
    "    punc_removed = strip_punctuation(lowered_string)\n",
    "    remove_whitespace = strip_multiple_whitespaces(punc_removed)\n",
    "    cleaned_string = re.sub(year_pattern, '', remove_whitespace)\n",
    "    # Gensim wants the sentences in list of strings [\"word1\", \"word2\", \"word3\", etc] format, so we can do that when returning!\n",
    "    return cleaned_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = [clean_sent(sent) for sent in abstracts]\n",
    "# Let's take a peak of the cleaned data\n",
    "cleaned_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & saving our model üèãüèΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only required parameter here is your text data. \n",
    "# The defaults for workers, epochs, min_count, etc are quite good. I just wanted to show what modifying them might look like.\n",
    "model = gensim.models.Word2Vec(sentences=cleaned_sentences, workers=6, epochs=1000, min_count=2, vector_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our model is that easy! Now lets see what words are in the model vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_word = random.choice(model.wv.index_to_key)\n",
    "random_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving is pretty easy too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/word2vec_tutorial.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load our newly trained model to see if it still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = gensim.models.Word2Vec.load('models/word2vec_tutorial.pkl')\n",
    "random_word = random.choice(loaded_model.wv.index_to_key)\n",
    "random_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! In the next tutorial we'll explore techniques to visualize our embedding space and better understand it. \n",
    "\n",
    "To recap we covered:\n",
    "- What are word embeddings.\n",
    "- Applications for word embeddings.\n",
    "- How you can train your own word embedding model (Word2Vec) using the Python library Gensim.\n",
    "- Text cleaning methods & considering your domain.\n",
    "- A wee tiny bit of linguistics.\n",
    "\n",
    "\n",
    "Thanks for reading! üòÅ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3c51b43522ba3c2e58abf5dfea2f8e8cf9bb1335a882ef9e750ae84018be2fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
