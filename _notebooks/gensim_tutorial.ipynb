{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create your own Word2Vec model for your domain\n",
    "This tutorial will cover:\n",
    "- What are word embeddings\n",
    "- Applications for word embeddings\n",
    "- How you can train your own word embedding model (Word2Vec) using the Python library Gensim\n",
    "- Text cleaning methods & considering your domain\n",
    "- Visualizing your embedding space\n",
    "\n",
    "We will be training a Word2Vec model on scientific abstracts taken from the Semantic Scholar Graph API! This process is very similar to the work I am doing in building ClimateScholar (an open source climate science literature search engine).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are word embeddings?\n",
    "\n",
    "\n",
    "## Why should you care?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Gensim?\n",
    "\n",
    "## Tutorial versions\n",
    "Just so you can follow along for reproducibility reasons, here are the versions for Python & the packages:\n",
    "- Python:\n",
    "- Gensim:\n",
    "- Jupyter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_multiple_whitespaces, strip_punctuation\n",
    "import json\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "root_path = \"./data/\"\n",
    "sample_data = [\"weather_CO2.jsonl\", \"paleoclimate.jsonl\", \"rewilding.jsonl\", \"rockfish.jsonl\", \"arctic.jsonl\", \"climate.jsonl\", \"shark_climate.jsonl\"]\n",
    "\n",
    "for data_path in sample_data:\n",
    "    with open(f'{root_path}/{data_path}', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    result = json.loads(json_list[0])\n",
    "\n",
    "    for result_dict in result[\"data\"]:\n",
    "        papers.append(result_dict)\n",
    "\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [y for y in (x for x in papers) if y[\"abstract\"] is not None]\n",
    "abstracts = [item['abstract'] for item in data]\n",
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_pattern = r'20[0-9]'\n",
    "def clean_sent(sent):\n",
    "    removed_stopwords = remove_stopwords(sent)\n",
    "    lowered_string = removed_stopwords.lower()\n",
    "    punc_removed = strip_punctuation(lowered_string)\n",
    "    remove_whitespace = strip_multiple_whitespaces(punc_removed)\n",
    "    cleaned_string = re.sub(year_pattern, '', remove_whitespace)\n",
    "    return cleaned_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = [clean_sent(sent) for sent in abstracts]\n",
    "cleaned_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=cleaned_sentences, workers=6, epochs=1000, min_count=2, vector_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_word = random.choice(model.wv.index_to_key)\n",
    "random_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/word2vec_first_pass.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = gensim.models.Word2Vec.load('models/word2vec_first_pass.pkl')\n",
    "random_word = random.choice(loaded_model.wv.index_to_key)\n",
    "random_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3c51b43522ba3c2e58abf5dfea2f8e8cf9bb1335a882ef9e750ae84018be2fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
