---
layout: post
description: A paper summary of A Frustratingly Easy Approach for Entity and Relation Extraction by Zexuan Zhong, Danqi Chen
categories: [nlp]
title: A Frustratingly Easy Approach for Entity and Relation Extraction [Paper Summary]
---

# A Frustratingly Easy Approach for Entity and Relation Extraction
Paper can be found [here.](https://arxiv.org/abs/2010.12812v2)

## What
Researchers at Princeton use two transformer language models to perform end-to-end relation extraction. (E2E-RE) Which involves identifying named entities in a text & extracting important relational information between entities.

The code for this paper can be found [here.](https://github.com/princeton-nlp/PURE)

## Why
Named Entity Recognition (NER) and Relation Extraction (RE) are two hugely important tasks in NLP. The applications of these techniques form the basis for a lot of modern technologies, and making these techniques more accessible can enable innovations in a lot more sectors. Relation extraction is one task I've seen neglected by many of the "accessible" AI frameworks. (That seems to be changing though)


## How
Recent work in E2E-RE has involved passing an input text into a single model, and the model outputs the extracted relations. Prior to this paradigm researchers would use several models to accomplish this task. The authors of this paper propose a return to form. 

They train two transformer language models. One known as the "entity model" to extract entities. The second known as the "relation model" to extract relations. The relation model's input is provided by the entity model. This helps pass important entity information through the system. The tweek manages to keep the system simple and high performing.

Entity Model:
- Builds on a span-level represntation.
- pre-trained BERT
- Takes an input sentence and predicts an entity type for each span. 

Relation Model:
- Builds on a contextual representation for a specific pair of spans. 
- Entity information (boundary & type) is passed through at the input layer. 
    - These are represented as additional marker tokens that represent the information predicted by the entity model (originally passed as subject-object spans)

## Key Findings
- Keep it simple(?)
- In the tradition of larger models being better, ALBERT implementations outperform smaller BERT models. 
- Models that specialize outperform generalists. 
- While some of the ideas used are not entirely new, the system as a whole is novel. 

## Open Questions
- How well does this approach scale to more jargon or technical domains? (i.e: biomedical, engineering)

## My perspective
This has many of the hallmarks of a great paper. It explains the problem & background information well. Is well written, provides implementation details & links to a well documented codebase. I already have plans to begin bootstraping the code they link for my own appplications. 